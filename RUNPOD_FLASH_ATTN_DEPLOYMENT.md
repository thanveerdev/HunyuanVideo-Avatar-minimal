# üöÄ RunPod Deployment with Flash Attention Fix

This guide provides **complete solutions** for the critical errors found in **My Pods Logs (29).txt** and ensures successful RunPod deployment.

## üö® Problems This Solves

### PRIMARY ERROR (FIXED ‚úÖ):
```
ModuleNotFoundError: No module named 'flash_attn'
```

### Additional Issues (FIXED ‚úÖ):
- TorchVision compatibility errors
- Missing gradio, fastapi, diffusers dependencies
- Gradio client schema errors
- CUDA/cuDNN optimization issues

---

## üõ†Ô∏è Complete Fix Implementation

### **Option 1: Use Flash Attention Docker Image (RECOMMENDED)**

The project now includes a **complete Flash Attention Docker setup**:

```bash
# Build the Flash Attention optimized image
./build_flash_attn_docker.sh

# Or use docker-compose
docker-compose -f docker-compose.flash-attn.yml up --build
```

**Key Features:**
- ‚úÖ **nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04** base image
- ‚úÖ **Flash Attention 2.6.3** pre-compiled
- ‚úÖ **PyTorch 2.4.0** with CUDA 12.4 support
- ‚úÖ All missing dependencies included
- ‚úÖ TorchVision compatibility fixes applied
- ‚úÖ Comprehensive error testing built-in

### **Option 2: Manual Installation (If needed)**

If you must install manually:

```bash
# Install Flash Attention (requires CUDA dev tools)
pip install flash-attn==2.6.3 --no-build-isolation

# Install missing dependencies
pip install -r requirements.txt

# Apply TorchVision fixes
python apply_torchvision_fix.py
```

---

## üèÉ‚Äç‚ôÇÔ∏è RunPod Deployment Steps

### **Step 1: Build and Push Docker Image**

```bash
# 1. Build the Flash Attention image locally
./build_flash_attn_docker.sh

# 2. Tag for Docker Hub (replace with your username)
docker tag hunyuan-video-avatar-flash-attn:cuda12.4-cudnn-ubuntu22.04 \
  yourusername/hunyuan-video-avatar-flash-attn:latest

# 3. Push to Docker Hub
docker push yourusername/hunyuan-video-avatar-flash-attn:latest
```

### **Step 2: Update RunPod Template**

The `runpod_template.json` has been updated with:
- ‚úÖ Correct Docker image with Flash Attention
- ‚úÖ Required environment variables
- ‚úÖ Optimized CUDA settings
- ‚úÖ Memory management configurations

### **Step 3: Deploy on RunPod**

1. **Create Pod with Template:**
   ```json
   {
     "dockerImage": "yourusername/hunyuan-video-avatar-flash-attn:cuda12.4-cudnn-ubuntu22.04",
     "containerDiskInGb": 50,
     "volumeInGb": 50,
     "ports": "7860/http,80/http"
   }
   ```

2. **Required Environment Variables:**
   ```bash
   FLASH_ATTENTION_FORCE_BUILD=TRUE
   FLASH_ATTENTION_FORCE_CUT=TRUE
   TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0
   CUDA_HOME=/usr/local/cuda
   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
   ```

3. **GPU Requirements:**
   - Minimum: RTX 3060 8GB
   - Recommended: RTX 3080+ 12GB+
   - Optimal: RTX 4090 24GB

---

## üß™ Testing Your Deployment

### **Built-in Error Tests**

The image includes comprehensive testing:

```bash
# Test Flash Attention (should work now)
python3 test_flash_attn_error.py

# Test all error persistence from logs
./run_error_persistence_tests.sh

# Test Gradio interface
python3 test_gradio_error.py
```

### **Expected Results After Fix:**
```
‚úÖ SUCCESS: The flash_attn error has been RESOLVED!
‚úÖ The application should now be able to start properly.
‚úÖ Flash Attention successfully installed!
‚úÖ All imports in chain completed successfully
```

### **Manual Verification:**
```bash
# Should work without errors:
python3 -c "import flash_attn; print('Flash Attention version:', flash_attn.__version__)"
python3 -c "from hymm_sp.modules.models_audio import HYVideoDiffusionTransformer; print('‚úÖ Models import successfully')"
```

---

## üîß Configuration Options

### **VRAM Optimization**
The deployment automatically detects and optimizes for available VRAM:

- **8GB (RTX 3060)**: Ultra-low mode, 256px, 16 frames
- **12GB (RTX 3080)**: Balanced mode, 512px, 64 frames  
- **16GB (RTX 4080)**: High performance, 704px, 128 frames
- **24GB (RTX 4090)**: Maximum quality, 1024px, 256 frames

### **Manual Override Environment Variables:**
```bash
VRAM_MODE=high_performance
IMAGE_SIZE=704
VIDEO_LENGTH=128
INFERENCE_STEPS=30
CPU_OFFLOAD=false
MIXED_PRECISION=true
```

---

## üìä Deployment Checklist

### **Before Deployment:**
- ‚úÖ Flash Attention Docker image built
- ‚úÖ Image pushed to Docker Hub/registry
- ‚úÖ RunPod template updated
- ‚úÖ Environment variables configured
- ‚úÖ GPU requirements met

### **After Deployment:**
- ‚úÖ Container starts without errors
- ‚úÖ Flash Attention imports successfully
- ‚úÖ Web interface accessible on port 7860
- ‚úÖ FastAPI backend running on port 80
- ‚úÖ Model weights download automatically
- ‚úÖ Memory optimization applied

### **Success Indicators:**
```
‚úÖ GPU detected with XXXXMB memory
‚úÖ High GPU memory detected (XXXXMB). Using extreme optimization mode
‚úÖ Applied HunyuanVideoGP-style extreme memory optimizations
‚úÖ TorchVision compatibility fix applied successfully
‚úÖ Configuration test passed
‚úÖ Web interface mode - starting Gradio UI
‚úÖ FastAPI server started
‚úÖ Flash Attention successfully installed!
```

---

## üö® Troubleshooting

### **If Flash Attention Still Fails:**
1. Check CUDA version: `nvidia-smi`
2. Verify PyTorch CUDA: `python -c "import torch; print(torch.cuda.is_available())"`
3. Rebuild image: `./build_flash_attn_docker.sh`
4. Check GPU compatibility: Compute Capability 8.0+

### **If TorchVision Errors Persist:**
1. Apply fix manually: `python apply_torchvision_fix.py`
2. Check transformers version: `pip show transformers`
3. Force reinstall: `pip install --force-reinstall torchvision==0.19.0`

### **If Memory Issues Occur:**
1. Reduce batch size in config
2. Enable CPU offload: `CPU_OFFLOAD=true`
3. Increase swap: `--shm-size=8g`
4. Use lower VRAM mode

---

## üìà Performance Expectations

### **Build Time:**
- **Flash Attention compilation**: 20-30 minutes
- **Docker image size**: ~8GB
- **First model download**: 10-30 minutes

### **Runtime Performance:**
- **Startup time**: 2-5 minutes
- **Generation speed**: Depends on VRAM mode
- **Memory usage**: Optimized for available GPU memory

---

## üéâ Success!

Once deployed successfully, you'll have:
- ‚úÖ **Working Flash Attention** - No more import errors
- ‚úÖ **Optimized VRAM usage** - Automatic GPU detection
- ‚úÖ **Web interface** - Access via RunPod's public URL
- ‚úÖ **FastAPI backend** - Robust processing pipeline
- ‚úÖ **Persistent outputs** - Network volume storage
- ‚úÖ **Error-free startup** - All dependencies resolved

**Your HunyuanVideo-Avatar is now ready for production on RunPod!** üöÄ

---

## üìû Support

If you encounter issues:
1. Check the built-in test scripts
2. Review container logs for specific errors
3. Verify GPU compatibility and VRAM
4. Ensure Docker image includes Flash Attention

**All critical errors from My Pods Logs (29).txt should now be resolved!** 